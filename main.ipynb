{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a1e9cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5dbca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the datasets\n",
    "clean_dataset = np.loadtxt('Dataset/clean_dataset.txt')\n",
    "noisy_dataset = np.loadtxt('Dataset/noisy_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a74bec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8)\n"
     ]
    }
   ],
   "source": [
    "print(clean_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "68a0d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8)\n"
     ]
    }
   ],
   "source": [
    "print(noisy_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c657f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-61. -60. -68. -62. -77. -90. -80.   1.]\n",
      " [-63. -65. -60. -63. -77. -81. -87.   1.]]\n"
     ]
    }
   ],
   "source": [
    "print(clean_dataset[1:5][2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a03f2f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(clean_dataset[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "707a30c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[500 500 500 500]\n"
     ]
    }
   ],
   "source": [
    "nique, counts = np.unique(clean_dataset[:,-1], return_counts=True)\n",
    "# result = dict(zip(clean_dataset, counts))\n",
    "print(len(nique))\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "59a3d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function to calculate entropy\n",
    "def cal_entropy(dataset):\n",
    "\n",
    "    nique, counts = np.unique(dataset[:,-1], return_counts=True)\n",
    "\n",
    "    pk_values = []\n",
    "\n",
    "    # calculating pk values\n",
    "    for k in range(len(nique)):\n",
    "        pk = counts[k]/len(dataset[:, -1])\n",
    "        pk_values.append(pk)\n",
    "\n",
    "    entropy = 0\n",
    "    # calculating entropy based on pk values\n",
    "    for pk in pk_values:\n",
    "        entropy += (pk * np.log2(pk))\n",
    "\n",
    "    return -1 * entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "30943eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the best split feature and value\n",
    "# We will iterate through all possible split points\n",
    "def find_split(dataset):\n",
    "\n",
    "    # Initialisation\n",
    "    best_info_gain = -9999\n",
    "    best_feature_index = None\n",
    "    best_feature_value = None\n",
    "\n",
    "    best_left_data = []\n",
    "    best_right_data = []\n",
    "\n",
    "    best_value = None\n",
    "\n",
    "    entropy_complete_dataset = cal_entropy(dataset)\n",
    "\n",
    "    # Loop through all features\n",
    "    for feature_index in range(dataset.shape[1]-1):\n",
    "        # Get unique feature values\n",
    "        unique_feature_values = np.unique(dataset[:,feature_index])\n",
    "\n",
    "        # Find all potential split points (midpoint between the values)\n",
    "        potential_splits = []\n",
    "\n",
    "        for i in range(len(unique_feature_values)-1):\n",
    "            current_value = unique_feature_values[i]\n",
    "            next_value = unique_feature_values[i+1]\n",
    "\n",
    "            mid_value = (current_value+next_value)/2\n",
    "\n",
    "            potential_splits.append(mid_value)\n",
    "\n",
    "        # For every value in the potential splits, we calculate left and right datasets\n",
    "        # we also calculate the information gain\n",
    "        # we keep track of the best split value and attribute, along with the best left and right datasets\n",
    "        for split_value in potential_splits:\n",
    "            left_dataset_list = []\n",
    "            right_dataset_list = []\n",
    "\n",
    "            for row in dataset:\n",
    "                if row[feature_index] <= split_value:\n",
    "                    left_dataset_list.append(row)\n",
    "                else:\n",
    "                    right_dataset_list.append(row)\n",
    "            \n",
    "            left_dataset = np.array(left_dataset_list)\n",
    "            right_dataset = np.array(right_dataset_list)\n",
    "\n",
    "            prob_left = len(left_dataset) / len(dataset)\n",
    "            prob_right = len(right_dataset) / len(dataset)\n",
    "\n",
    "            information_gain = entropy_complete_dataset - (prob_left * cal_entropy(left_dataset) + (prob_right * cal_entropy(right_dataset)))\n",
    " \n",
    "            if information_gain > best_info_gain:\n",
    "                best_info_gain = information_gain\n",
    "                best_feature_index = feature_index\n",
    "                \n",
    "                best_left_data = left_dataset\n",
    "                best_right_data = right_dataset\n",
    "\n",
    "                best_value = split_value\n",
    "\n",
    "    return best_info_gain, best_feature_index, best_value, best_left_data, best_right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "87e445e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our node class\n",
    "# Has left and right values which point to different nodes\n",
    "# The split value and attribute index mentions what value and which attribute index we are going to be splitting on\n",
    "# If the node is a leaf, isLeaf would be true and prediction would return the prediction for that node\n",
    "class Node:\n",
    "  def __init__(self, left = None, right = None, splitValue = None, attributeIndex = None,\n",
    "   isLeaf = False, prediction = None) -> None:\n",
    "    self.left = left\n",
    "    self.right = right\n",
    "    self.splitValue = splitValue\n",
    "    self.attributeIndex = attributeIndex\n",
    "    self.isLeaf = isLeaf\n",
    "    self.prediction = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bb98e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_learning(dataset, depth):\n",
    "\n",
    "     # if the data has all the same values, then we have a leaf node\n",
    "    unique_labels = np.unique(dataset[:,-1])\n",
    "    if len(unique_labels)==1:\n",
    "        leafNode = Node(isLeaf=True, prediction=unique_labels[0])\n",
    "        return leafNode, depth\n",
    "\n",
    "    # otherwise, we get the best values for split, and create a new node\n",
    "    best_info_gain, best_feature_index, best_value, left_data, right_data = find_split(dataset)\n",
    "\n",
    "    # Best feature index \n",
    "    # best info gain\n",
    "    # best feature value\n",
    "    node = Node(splitValue=best_value,\n",
    "                attributeIndex=best_feature_index)\n",
    "    node.left, l_depth = decision_tree_learning(left_data, depth + 1)\n",
    "    node.right, r_depth = decision_tree_learning(right_data, depth + 1)\n",
    "\n",
    "    return (node, max(l_depth, r_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2dd72afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to shuffle data indices and split values\n",
    "def kfold_datasets_generator(dataset):\n",
    "\n",
    "    working_data = dataset.copy()\n",
    "    print(working_data.shape)\n",
    "    np.random.shuffle(working_data)\n",
    "    split_datasets = np.split(working_data, 10)\n",
    "    print(len(split_datasets))\n",
    "    print(split_datasets[0].shape)\n",
    "\n",
    "    # Okay, now we have 10 datasets. We need to focus on training the tree on the first 9 and evaluate on the last 1\n",
    "\n",
    "    l = []\n",
    "    for i in range(len(split_datasets)):\n",
    "        # The ith set will be the testing set, the rest will be the training set\n",
    "        testing_set = split_datasets[i]\n",
    "        training_set = split_datasets[:i] + split_datasets[i + 1:]\n",
    "\n",
    "        training_set = np.concatenate((training_set), axis = 0)\n",
    "\n",
    "        d = {'testing': testing_set, 'training': training_set}\n",
    "        l.append(d)\n",
    "    \n",
    "    # this function returns a list of 10 dictionaries, with each dictionary having a 'training' and 'testing' key\n",
    "    # the keys point to their respective training and testing dataset\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4d546e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(node: Node, input):\n",
    "\n",
    "    # given an input of size (1, 8), we follow the decision tree, and return the final predicted value\n",
    "    val = node.splitValue\n",
    "    index = node.attributeIndex\n",
    "    if node.isLeaf:\n",
    "        return node.prediction\n",
    "    if input[index] <= val:\n",
    "        return get_prediction(node.left, input)\n",
    "    else:\n",
    "        return get_prediction(node.right, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ad4b3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_evaluator(datasets):\n",
    "\n",
    "    # to track the accuracy of all of our folds\n",
    "    all_fold_accuracies = []\n",
    "    \n",
    "    \n",
    "    for i in datasets:\n",
    "        # we loop through every dataset and train a decision tree on the training data\n",
    "        node, depth = decision_tree_learning(i['training'], 0)\n",
    "\n",
    "        confusion_matrix = [[0, 0, 0, 0],[0, 0, 0, 0],[0, 0, 0, 0],[0, 0, 0, 0]]\n",
    "\n",
    "        # for every row in the training data, we get a predicted and actual value, then build our confusion matrix based on that\n",
    "        for j in range(len(i['testing'])):\n",
    "            predicted = get_prediction(node, i['testing'][j])\n",
    "            actual = i['testing'][j][-1]\n",
    "            predicted.astype(np.int64)\n",
    "            predicted = predicted.astype(np.int64)\n",
    "            actual = actual.astype(np.int64)\n",
    "            confusion_matrix[predicted - 1][actual - 1] += 1\n",
    "        confusion_matrix = np.array(confusion_matrix)\n",
    "        print(confusion_matrix)\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # --- Integrated Statistics Calculation ---\n",
    "        \n",
    "        print(\"--- Per-Class Statistics ---\")\n",
    "        \n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for k in range(4): # Use 'k' to avoid shadowing the outer 'i'\n",
    "            class_name = f\"Class {k+1}\"\n",
    "\n",
    "            # True Positives: Correctly predicted for this class\n",
    "            true_positives = confusion_matrix[k, k]\n",
    "            \n",
    "            # False Positives: Predicted as this class, but was actually another\n",
    "            # Sum of row 'k', minus the TP\n",
    "            false_positives = np.sum(confusion_matrix[k, :]) - true_positives\n",
    "            \n",
    "            # False Negatives: Actually this class, but predicted as another\n",
    "            # Sum of column 'k', minus the TP\n",
    "            false_negatives = np.sum(confusion_matrix[:, k]) - true_positives\n",
    "\n",
    "            # --- Calculate Precision ---\n",
    "            # TP / (TP + FP)\n",
    "            precision_denominator = (true_positives + false_positives)\n",
    "            if precision_denominator == 0:\n",
    "                precision = 0.0\n",
    "            else:\n",
    "                precision = true_positives / precision_denominator\n",
    "            \n",
    "            precision_scores.append(precision)\n",
    "\n",
    "            # --- Calculate Recall ---\n",
    "            # TP / (TP + FN)\n",
    "            recall_denominator = (true_positives + false_negatives)\n",
    "            if recall_denominator == 0:\n",
    "                recall = 0.0\n",
    "            else:\n",
    "                recall = true_positives / recall_denominator\n",
    "            \n",
    "            recall_scores.append(recall)\n",
    "\n",
    "            print(f\"{class_name}:\")\n",
    "            print(f\"  True Positives:  {true_positives}\")\n",
    "            print(f\"  False Positives: {false_positives}\")\n",
    "            print(f\"  False Negatives: {false_negatives}\")\n",
    "            print(f\"  Precision:       {precision:.4f}\")\n",
    "            print(f\"  Recall:          {recall:.4f}\")\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(\"--- Overall Statistics (for this fold) ---\")\n",
    "        \n",
    "        # Overall Accuracy = (Sum of all correct) / (Total samples)\n",
    "        total_correct = np.trace(confusion_matrix) # Sum of the diagonal\n",
    "        total_samples = np.sum(confusion_matrix)\n",
    "        \n",
    "        if total_samples == 0:\n",
    "            overall_accuracy = 0.0\n",
    "        else:\n",
    "            overall_accuracy = total_correct / total_samples\n",
    "            \n",
    "        all_fold_accuracies.append(overall_accuracy)\n",
    "\n",
    "        print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "        print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "        print(\"=\" * 30 + \"\\n\")\n",
    "\n",
    "\n",
    "    print(\"--- Final K-Fold Summary ---\")\n",
    "    if len(all_fold_accuracies) > 0:\n",
    "        print(f\"Average Overall Accuracy across all folds: {np.mean(all_fold_accuracies):.4f}\")\n",
    "    else:\n",
    "        print(\"No datasets were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6e8658f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8)\n",
      "10\n",
      "(200, 8)\n",
      "[[41  4  3  6]\n",
      " [ 0 51  2  2]\n",
      " [ 4  3 37  6]\n",
      " [ 0  3  3 35]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  41\n",
      "  False Positives: 13\n",
      "  False Negatives: 4\n",
      "  Precision:       0.7593\n",
      "  Recall:          0.9111\n",
      "Class 2:\n",
      "  True Positives:  51\n",
      "  False Positives: 4\n",
      "  False Negatives: 10\n",
      "  Precision:       0.9273\n",
      "  Recall:          0.8361\n",
      "Class 3:\n",
      "  True Positives:  37\n",
      "  False Positives: 13\n",
      "  False Negatives: 8\n",
      "  Precision:       0.7400\n",
      "  Recall:          0.8222\n",
      "Class 4:\n",
      "  True Positives:  35\n",
      "  False Positives: 6\n",
      "  False Negatives: 14\n",
      "  Precision:       0.8537\n",
      "  Recall:          0.7143\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.8200\n",
      "Average Precision: 0.8200\n",
      "Average Recall: 0.8209\n",
      "==============================\n",
      "\n",
      "[[39  1  0  1]\n",
      " [ 4 48  2  2]\n",
      " [ 2  3 47  2]\n",
      " [ 3  3  1 42]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  39\n",
      "  False Positives: 2\n",
      "  False Negatives: 9\n",
      "  Precision:       0.9512\n",
      "  Recall:          0.8125\n",
      "Class 2:\n",
      "  True Positives:  48\n",
      "  False Positives: 8\n",
      "  False Negatives: 7\n",
      "  Precision:       0.8571\n",
      "  Recall:          0.8727\n",
      "Class 3:\n",
      "  True Positives:  47\n",
      "  False Positives: 7\n",
      "  False Negatives: 3\n",
      "  Precision:       0.8704\n",
      "  Recall:          0.9400\n",
      "Class 4:\n",
      "  True Positives:  42\n",
      "  False Positives: 7\n",
      "  False Negatives: 5\n",
      "  Precision:       0.8571\n",
      "  Recall:          0.8936\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.8800\n",
      "Average Precision: 0.8840\n",
      "Average Recall: 0.8797\n",
      "==============================\n",
      "\n",
      "[[42  6  5  7]\n",
      " [ 2 41  5  3]\n",
      " [ 2  2 33  2]\n",
      " [ 3  2  2 43]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  42\n",
      "  False Positives: 18\n",
      "  False Negatives: 7\n",
      "  Precision:       0.7000\n",
      "  Recall:          0.8571\n",
      "Class 2:\n",
      "  True Positives:  41\n",
      "  False Positives: 10\n",
      "  False Negatives: 10\n",
      "  Precision:       0.8039\n",
      "  Recall:          0.8039\n",
      "Class 3:\n",
      "  True Positives:  33\n",
      "  False Positives: 6\n",
      "  False Negatives: 12\n",
      "  Precision:       0.8462\n",
      "  Recall:          0.7333\n",
      "Class 4:\n",
      "  True Positives:  43\n",
      "  False Positives: 7\n",
      "  False Negatives: 12\n",
      "  Precision:       0.8600\n",
      "  Recall:          0.7818\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.7950\n",
      "Average Precision: 0.8025\n",
      "Average Recall: 0.7941\n",
      "==============================\n",
      "\n",
      "[[46  4  6  3]\n",
      " [ 4 35  4  1]\n",
      " [ 0  3 44  4]\n",
      " [ 4  4  6 32]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  46\n",
      "  False Positives: 13\n",
      "  False Negatives: 8\n",
      "  Precision:       0.7797\n",
      "  Recall:          0.8519\n",
      "Class 2:\n",
      "  True Positives:  35\n",
      "  False Positives: 9\n",
      "  False Negatives: 11\n",
      "  Precision:       0.7955\n",
      "  Recall:          0.7609\n",
      "Class 3:\n",
      "  True Positives:  44\n",
      "  False Positives: 7\n",
      "  False Negatives: 16\n",
      "  Precision:       0.8627\n",
      "  Recall:          0.7333\n",
      "Class 4:\n",
      "  True Positives:  32\n",
      "  False Positives: 14\n",
      "  False Negatives: 8\n",
      "  Precision:       0.6957\n",
      "  Recall:          0.8000\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.7850\n",
      "Average Precision: 0.7834\n",
      "Average Recall: 0.7865\n",
      "==============================\n",
      "\n",
      "[[31  4  4  4]\n",
      " [ 5 28  6  7]\n",
      " [ 7  4 41  2]\n",
      " [ 7  2  4 44]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  31\n",
      "  False Positives: 12\n",
      "  False Negatives: 19\n",
      "  Precision:       0.7209\n",
      "  Recall:          0.6200\n",
      "Class 2:\n",
      "  True Positives:  28\n",
      "  False Positives: 18\n",
      "  False Negatives: 10\n",
      "  Precision:       0.6087\n",
      "  Recall:          0.7368\n",
      "Class 3:\n",
      "  True Positives:  41\n",
      "  False Positives: 13\n",
      "  False Negatives: 14\n",
      "  Precision:       0.7593\n",
      "  Recall:          0.7455\n",
      "Class 4:\n",
      "  True Positives:  44\n",
      "  False Positives: 13\n",
      "  False Negatives: 13\n",
      "  Precision:       0.7719\n",
      "  Recall:          0.7719\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.7200\n",
      "Average Precision: 0.7152\n",
      "Average Recall: 0.7186\n",
      "==============================\n",
      "\n",
      "[[46  3  2  3]\n",
      " [ 3 45  2  3]\n",
      " [ 4  3 39  6]\n",
      " [ 3  0  2 36]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  46\n",
      "  False Positives: 8\n",
      "  False Negatives: 10\n",
      "  Precision:       0.8519\n",
      "  Recall:          0.8214\n",
      "Class 2:\n",
      "  True Positives:  45\n",
      "  False Positives: 8\n",
      "  False Negatives: 6\n",
      "  Precision:       0.8491\n",
      "  Recall:          0.8824\n",
      "Class 3:\n",
      "  True Positives:  39\n",
      "  False Positives: 13\n",
      "  False Negatives: 6\n",
      "  Precision:       0.7500\n",
      "  Recall:          0.8667\n",
      "Class 4:\n",
      "  True Positives:  36\n",
      "  False Positives: 5\n",
      "  False Negatives: 12\n",
      "  Precision:       0.8780\n",
      "  Recall:          0.7500\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.8300\n",
      "Average Precision: 0.8322\n",
      "Average Recall: 0.8301\n",
      "==============================\n",
      "\n",
      "[[36  3  7  1]\n",
      " [ 3 43  4  3]\n",
      " [ 6 10 37  3]\n",
      " [ 4  2  3 35]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  36\n",
      "  False Positives: 11\n",
      "  False Negatives: 13\n",
      "  Precision:       0.7660\n",
      "  Recall:          0.7347\n",
      "Class 2:\n",
      "  True Positives:  43\n",
      "  False Positives: 10\n",
      "  False Negatives: 15\n",
      "  Precision:       0.8113\n",
      "  Recall:          0.7414\n",
      "Class 3:\n",
      "  True Positives:  37\n",
      "  False Positives: 19\n",
      "  False Negatives: 14\n",
      "  Precision:       0.6607\n",
      "  Recall:          0.7255\n",
      "Class 4:\n",
      "  True Positives:  35\n",
      "  False Positives: 9\n",
      "  False Negatives: 7\n",
      "  Precision:       0.7955\n",
      "  Recall:          0.8333\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.7550\n",
      "Average Precision: 0.7584\n",
      "Average Recall: 0.7587\n",
      "==============================\n",
      "\n",
      "[[36  6  5  5]\n",
      " [ 8 33  3  0]\n",
      " [ 4  3 41  6]\n",
      " [ 3  3  2 42]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  36\n",
      "  False Positives: 16\n",
      "  False Negatives: 15\n",
      "  Precision:       0.6923\n",
      "  Recall:          0.7059\n",
      "Class 2:\n",
      "  True Positives:  33\n",
      "  False Positives: 11\n",
      "  False Negatives: 12\n",
      "  Precision:       0.7500\n",
      "  Recall:          0.7333\n",
      "Class 3:\n",
      "  True Positives:  41\n",
      "  False Positives: 13\n",
      "  False Negatives: 10\n",
      "  Precision:       0.7593\n",
      "  Recall:          0.8039\n",
      "Class 4:\n",
      "  True Positives:  42\n",
      "  False Positives: 8\n",
      "  False Negatives: 11\n",
      "  Precision:       0.8400\n",
      "  Recall:          0.7925\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.7600\n",
      "Average Precision: 0.7604\n",
      "Average Recall: 0.7589\n",
      "==============================\n",
      "\n",
      "[[32  3  2  6]\n",
      " [ 3 43  0  1]\n",
      " [ 2  5 49  3]\n",
      " [ 6  2  7 36]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  32\n",
      "  False Positives: 11\n",
      "  False Negatives: 11\n",
      "  Precision:       0.7442\n",
      "  Recall:          0.7442\n",
      "Class 2:\n",
      "  True Positives:  43\n",
      "  False Positives: 4\n",
      "  False Negatives: 10\n",
      "  Precision:       0.9149\n",
      "  Recall:          0.8113\n",
      "Class 3:\n",
      "  True Positives:  49\n",
      "  False Positives: 10\n",
      "  False Negatives: 9\n",
      "  Precision:       0.8305\n",
      "  Recall:          0.8448\n",
      "Class 4:\n",
      "  True Positives:  36\n",
      "  False Positives: 15\n",
      "  False Negatives: 10\n",
      "  Precision:       0.7059\n",
      "  Recall:          0.7826\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.8000\n",
      "Average Precision: 0.7989\n",
      "Average Recall: 0.7957\n",
      "==============================\n",
      "\n",
      "[[40  4  4  3]\n",
      " [ 0 28  6  3]\n",
      " [ 4  4 42  5]\n",
      " [ 1  3  3 50]]\n",
      "------------------------------\n",
      "--- Per-Class Statistics ---\n",
      "Class 1:\n",
      "  True Positives:  40\n",
      "  False Positives: 11\n",
      "  False Negatives: 5\n",
      "  Precision:       0.7843\n",
      "  Recall:          0.8889\n",
      "Class 2:\n",
      "  True Positives:  28\n",
      "  False Positives: 9\n",
      "  False Negatives: 11\n",
      "  Precision:       0.7568\n",
      "  Recall:          0.7179\n",
      "Class 3:\n",
      "  True Positives:  42\n",
      "  False Positives: 13\n",
      "  False Negatives: 13\n",
      "  Precision:       0.7636\n",
      "  Recall:          0.7636\n",
      "Class 4:\n",
      "  True Positives:  50\n",
      "  False Positives: 7\n",
      "  False Negatives: 11\n",
      "  Precision:       0.8772\n",
      "  Recall:          0.8197\n",
      "------------------------------\n",
      "--- Overall Statistics (for this fold) ---\n",
      "Overall Accuracy: 0.8000\n",
      "Average Precision: 0.7955\n",
      "Average Recall: 0.7975\n",
      "==============================\n",
      "\n",
      "--- Final K-Fold Summary ---\n",
      "Average Overall Accuracy across all folds: 0.7945\n"
     ]
    }
   ],
   "source": [
    "datasets = kfold_datasets_generator(noisy_dataset)\n",
    "kfold_evaluator(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
